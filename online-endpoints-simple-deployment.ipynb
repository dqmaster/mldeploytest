{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy and score a machine learning model by using an online endpoint \n",
        "\n",
        "Learn how to use an online endpoint to deploy your model, so you don't have to create and manage the underlying infrastructure. You'll begin by deploying a model on your local machine to debug any errors, and then you'll deploy and test it in Azure.\n",
        "\n",
        "Managed online endpoints help to deploy your ML models in a turnkey manner. Managed online endpoints work with powerful CPU and GPU machines in Azure in a scalable, fully managed way. Managed online endpoints take care of serving, scaling, securing, and monitoring your models, freeing you from the overhead of setting up and managing the underlying infrastructure. \n",
        "\n",
        "For more information, see [What are Azure Machine Learning endpoints?](https://docs.microsoft.com/azure/machine-learning/concept-endpoints)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "\n",
        "* To use Azure Machine Learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n",
        "\n",
        "* Install and configure the [Python SDK v2](sdk/setup.sh).\n",
        "\n",
        "* You must have an Azure resource group, and you (or the service principal you use) must have Contributor access to it.\n",
        "\n",
        "* You must have an Azure Machine Learning workspace. \n",
        "\n",
        "* To deploy locally, you must install Docker Engine on your local computer. We highly recommend this option, so it's easier to debug issues."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Connect to Azure Machine Learning Workspace\n",
        "\n",
        "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
        "\n",
        "## 1.1. Import the required libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        ")\n",
        "from azure.identity import DefaultAzureCredential"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1671473006812
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Configure workspace details and get a handle to the workspace\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# enter details of your AML workspace\n",
        "subscription_id = \"e9a02075-e822-4b24-95f3-ce725888b904\"\n",
        "resource_group = \"pricing-e-machine-learning-eastus-dev-rg\"\n",
        "workspace = \"e-machine-learning-eastus-dev-ws\""
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1671473015599
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1671473032274
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy and debug locally by using local endpoints"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note\n",
        "* To deploy locally, [Docker Engine](https://docs.docker.com/engine/install/) must be installed.\n",
        "* Docker Engine must be running. Docker Engine typically starts when the computer starts. If it doesn't, you can [troubleshoot Docker Engine](https://docs.docker.com/config/daemon/#start-the-daemon-manually)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create local endpoint and deployment\n",
        "\n",
        "## 2.1 Create local endpoint\n",
        "\n",
        "The goal of a local endpoint deployment is to validate and debug your code and configuration before you deploy to Azure. Local deployment has the following limitations:\n",
        "* Local endpoints *do not support* traffic rules, authentication, or probe settings.\n",
        "* Local endpoints support only one deployment per endpoint"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a local endpoint\n",
        "import datetime\n",
        "\n",
        "local_endpoint_name = \"local-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=local_endpoint_name, description=\"this is a sample local endpoint\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1671473061256
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_create_or_update(endpoint, local=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Creating local endpoint (local-12191804239098) .Done (0m 5s)\nField 'mirror_traffic': This is an experimental field, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "ManagedOnlineEndpoint({'public_network_access': None, 'provisioning_state': None, 'scoring_uri': None, 'swagger_uri': None, 'name': 'local-12191804239098', 'description': 'this is a sample local endpoint', 'tags': {}, 'properties': {}, 'id': None, 'base_path': './', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8778038d90>, 'auth_mode': 'key', 'location': None, 'identity': None, 'traffic': {}, 'mirror_traffic': {}, 'kind': None})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1671473081133
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Create local deployment\n",
        "\n",
        "The example contains all the files needed to deploy a model on an online endpoint. To deploy a model, you must have:\n",
        "\n",
        "* Model files (or the name and version of a model that's already registered in your workspace). In the example, we have a scikit-learn model that does regression.\n",
        "* The code that's required to score the model. In this case, we have a score.py file.\n",
        "* An environment in which your model runs. As you'll see, the environment might be a Docker image with Conda dependencies, or it might be a Dockerfile.\n",
        "* Settings to specify the instance type and scaling capacity.\n",
        "\n",
        "### Key aspects of deployment \n",
        "\n",
        "- `name` - Name of the deployment.\n",
        "- `endpoint_name` - Name of the endpoint to create the deployment under.\n",
        "- `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n",
        "- `environment` - The environment to use for the deployment. This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification.\n",
        "- `code_configuration` - the configuration for the source code and scoring script\n",
        "    - `path`- Path to the source code directory for scoring the model\n",
        "    - `scoring_script` - Relative path to the scoring file in the source code directory\n",
        "- `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](https://docs.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list).\n",
        "- `instance_count` - The number of instances to use for the deployment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(path=\"./model-1/model/sklearn_regression_model.pkl\")\n",
        "env = Environment(\n",
        "    conda_file=\"./model-1/environment/conda.yml\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n",
        ")\n",
        "\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=local_endpoint_name,\n",
        "    model=model,\n",
        "    environment=env,\n",
        "    code_configuration=CodeConfiguration(\n",
        "        code=\"./model-1/onlinescoring\", scoring_script=\"score.py\"\n",
        "    ),\n",
        "    instance_type=\"Standard_F2s_v2\",\n",
        "    instance_count=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1671473340090
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_deployments.begin_create_or_update(\n",
        "    deployment=blue_deployment, local=True\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Updating local deployment (local-12191804239098 / blue) .\nBuilding Docker image from Dockerfile\nStep 1/6 : FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\n ---> 9fab65be7722\nStep 2/6 : RUN mkdir -p /var/azureml-app/\n ---> Using cache\n ---> f6d8cf722139\nStep 3/6 : WORKDIR /var/azureml-app/\n ---> Using cache\n ---> 5f7e172624c5\nStep 4/6 : COPY conda.yml /var/azureml-app/\n ---> Using cache\n ---> 9413a786d623\nStep 5/6 : RUN conda env create -n inf-conda-env --file conda.yml\n ---> Using cache\n ---> a2a34c3ca9b4\nStep 6/6 : CMD [\"conda\", \"run\", \"--no-capture-output\", \"-n\", \"inf-conda-env\", \"runsvdir\", \"/var/runit\"]\n ---> Using cache\n ---> 7899ab1e5779\nSuccessfully built 7899ab1e5779\nSuccessfully tagged local-12191804239098:blue\n\nStarting up endpoint.....Done (0m 30s)\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "ManagedOnlineDeployment({'private_network_connection': None, 'egress_public_network_access': None, 'provisioning_state': 'Succeeded', 'endpoint_name': 'local-12191804239098', 'type': 'Managed', 'name': 'blue', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'base_path': PosixPath('/mnt/batch/tasks/shared/LS_root/mounts/clusters/zvealey-ci/code/Users/vea02153z/LocalEndpointTest'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8773112a60>, 'model': Model({'job_name': None, 'is_anonymous': False, 'auto_increment_version': False, 'name': '7713d7a5680d37a33a7ac52530aec294', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'base_path': PosixPath('/mnt/batch/tasks/shared/LS_root/mounts/clusters/zvealey-ci/code/Users/vea02153z/LocalEndpointTest'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f87731124c0>, 'version': '1', 'latest_version': None, 'path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/zvealey-ci/code/Users/vea02153z/LocalEndpointTest/model-1/model/sklearn_regression_model.pkl', 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model'}), 'code_configuration': <azure.ai.ml.entities._deployment.code_configuration.CodeConfiguration object at 0x7f8773112d90>, 'environment': Environment({'is_anonymous': False, 'auto_increment_version': False, 'name': 'CliV2AnonymousEnvironment', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'base_path': PosixPath('/mnt/batch/tasks/shared/LS_root/mounts/clusters/zvealey-ci/code/Users/vea02153z/LocalEndpointTest'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8773112ac0>, 'version': 'f53727d1df8137354bc0f2c69d2619ee', 'latest_version': None, 'conda_file': {'name': 'model-env', 'channels': ['conda-forge'], 'dependencies': ['python=3.7', 'numpy=1.21.2', 'pip=21.2.4', 'scikit-learn=0.24.2', 'scipy=1.7.1', {'pip': ['azureml-defaults==1.39.0', 'inference-schema[numpy-support]==1.3.0', 'joblib==1.0.1']}]}, 'image': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1', 'build': None, 'inference_config': None, 'os_type': None, 'arm_type': 'environment_version', 'conda_file_path': None, 'path': None, 'upload_hash': None, 'translated_conda_file': 'channels:\\n- conda-forge\\ndependencies:\\n- python=3.7\\n- numpy=1.21.2\\n- pip=21.2.4\\n- scikit-learn=0.24.2\\n- scipy=1.7.1\\n- pip:\\n  - azureml-defaults==1.39.0\\n  - inference-schema[numpy-support]==1.3.0\\n  - joblib==1.0.1\\nname: model-env\\n'}), 'environment_variables': {}, 'app_insights_enabled': False, 'scale_settings': None, 'request_settings': None, 'liveness_probe': None, 'readiness_probe': None, 'instance_count': 1, 'arm_type': 'online_deployment', 'model_mount_path': None, 'instance_type': 'local'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1671473370524
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Verify the local deployment succeeded\n",
        "\n",
        "## 3.1 Check the status to see whether the model was deployed without error"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.get(name=local_endpoint_name, local=True)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "ManagedOnlineEndpoint({'public_network_access': None, 'provisioning_state': 'Succeeded', 'scoring_uri': 'http://localhost:49158/score', 'swagger_uri': None, 'name': 'local-12191804239098', 'description': 'this is a sample local endpoint', 'tags': {}, 'properties': {}, 'id': None, 'base_path': './', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f87731301c0>, 'auth_mode': 'key', 'location': 'local', 'identity': None, 'traffic': {}, 'mirror_traffic': {}, 'kind': None})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1671473370710
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Get logs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_deployments.get_logs(\n",
        "    name=\"blue\", endpoint_name=local_endpoint_name, local=True, lines=50\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "\"2022-12-19T18:09:13,450957600+00:00 - iot-server/run \\r\\n2022-12-19T18:09:13,455273700+00:00 - rsyslog/run \\r\\n2022-12-19T18:09:13,456448100+00:00 - nginx/run \\r\\n2022-12-19T18:09:13,465295800+00:00 - gunicorn/run \\r\\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\\r\\n2022-12-19T18:09:13,538807200+00:00 - iot-server/finish 1 0\\r\\n2022-12-19T18:09:13,541496700+00:00 - Exit code 1 is normal. Not restarting iot-server.\\r\\nDynamic Python package installation is disabled.\\r\\nStarting HTTP server\\r\\nStarting gunicorn 20.1.0\\r\\nListening at: http://127.0.0.1:31311 (29)\\r\\nUsing worker: sync\\r\\nworker timeout is set to 300\\r\\nBooting worker with pid: 68\\r\\nSPARK_HOME not set. Skipping PySpark Initialization.\\r\\nInitializing logger\\r\\n2022-12-19 18:09:14,578 | root | INFO | Starting up app insights client\\r\\nlogging socket was found. logging is available.\\r\\nlogging socket was found. logging is available.\\r\\n2022-12-19 18:09:14,582 | root | INFO | Starting up request id generator\\r\\n2022-12-19 18:09:14,582 | root | INFO | Starting up app insight hooks\\r\\n2022-12-19 18:09:14,582 | root | INFO | Invoking user's init function\\r\\nInit complete\\r\\n2022-12-19 18:09:14,966 | root | INFO | Users's init has completed successfully\\r\\n2022-12-19 18:09:14,974 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\\r\\n2022-12-19 18:09:14,974 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\\r\\n2022-12-19 18:09:14,981 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\\r\\n\""
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1671473370870
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Invoke the local endpoint\n",
        "Invoke the endpoint to score the model by using the convenience command invoke and passing query parameters that are stored in a JSON file"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=local_endpoint_name,\n",
        "    request_file=\"./model-1/sample-request.json\",\n",
        "    local=True,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "'{\"auction_id\": 1, \"bid_cents\": 11055.977245525679}'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1671473371064
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Delete the endpoint\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_delete(name=local_endpoint_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "description": {
      "description": "Use an online endpoint to deploy your model, so you don't have to create and manage the underlying infrastructure"
    },
    "interpreter": {
      "hash": "819d9bd6416d6c156d06946f3add5943cbfa0a025710852352e8c572dce456f0"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}